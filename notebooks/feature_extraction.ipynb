{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b4a5ac-fb1d-49f5-9310-ab14aa175980",
   "metadata": {},
   "source": [
    "# Feature Extraction with Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b320c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee3ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073e522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 ubuntu hadoop 2807409271 2025-12-13 09:50 data/2019-08-22.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1564486e-b69f-4cf4-a208-7e3844a442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "spark_ui_port = 4040\n",
    "app_name = \"Otus\"\n",
    "\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bedeb1f-a0a1-4731-b115-b7619551ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/13 10:08:03 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/12/13 10:08:19 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/12/13 10:08:19 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession\n",
    "        .builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351d24fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rc1a-dataproc-m-021h626vnjpnod2k.mdb.yandexcloud.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Otus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee40a57520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f115989-7bef-4de3-83e7-ac9ad0674cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|tx_datetime        |customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|1            |2019-08-22 05:10:37|0          |0          |90.55    |18637          |0           |0       |0                |\n",
      "|2            |2019-08-22 19:05:33|0          |753        |35.38    |68733          |0           |0       |0                |\n",
      "|3            |2019-08-22 07:21:33|0          |0          |80.41    |26493          |0           |0       |0                |\n",
      "|4            |2019-08-22 09:06:17|1          |981        |102.83   |32777          |0           |0       |0                |\n",
      "|5            |2019-08-22 18:41:25|3          |205        |34.2     |67285          |0           |0       |0                |\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- tranaction_id: integer (nullable = true)\n",
      " |-- tx_datetime: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- terminal_id: integer (nullable = true)\n",
      " |-- tx_amount: double (nullable = true)\n",
      " |-- tx_time_seconds: integer (nullable = true)\n",
      " |-- tx_time_days: integer (nullable = true)\n",
      " |-- tx_fraud: integer (nullable = true)\n",
      " |-- tx_fraud_scenario: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"tranaction_id\", IntegerType(), True),\n",
    "    StructField(\"tx_datetime\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"terminal_id\", IntegerType(), True),\n",
    "    StructField(\"tx_amount\", DoubleType(), True),\n",
    "    StructField(\"tx_time_seconds\", IntegerType(), True),\n",
    "    StructField(\"tx_time_days\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"comment\", \"#\") \n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .schema(schema)\n",
    "    .load(\"hdfs:///user/ubuntu/data/*\")\n",
    ")\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0883f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"hdfs:///user/ubuntu/tx_parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8abab51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "937b099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 46988417)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset shape\n",
    "(len(df.columns), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f7125d-15b1-460e-9015-a185ad34116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tranaction_id: integer (nullable = true)\n",
      " |-- tx_datetime: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- terminal_id: integer (nullable = true)\n",
      " |-- tx_amount: double (nullable = true)\n",
      " |-- tx_time_seconds: integer (nullable = true)\n",
      " |-- tx_time_days: integer (nullable = true)\n",
      " |-- tx_fraud: integer (nullable = true)\n",
      " |-- tx_fraud_scenario: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753d965",
   "metadata": {},
   "source": [
    "Так смотреть не удобно - отформатируем вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4b2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tranaction_id\tint\n",
      "              tx_datetime\ttimestamp\n",
      "              customer_id\tint\n",
      "              terminal_id\tint\n",
      "                tx_amount\tdouble\n",
      "          tx_time_seconds\tint\n",
      "             tx_time_days\tint\n",
      "                 tx_fraud\tint\n",
      "        tx_fraud_scenario\tint\n"
     ]
    }
   ],
   "source": [
    "dt = df.dtypes\n",
    "for r in dt:\n",
    "    print(f\"{r[0]:>25}\\t{r[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f237e63",
   "metadata": {},
   "source": [
    "Давайте cортируем по типам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b559b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tx_amount\tdouble\n",
      "            tranaction_id\tint\n",
      "              customer_id\tint\n",
      "              terminal_id\tint\n",
      "          tx_time_seconds\tint\n",
      "             tx_time_days\tint\n",
      "                 tx_fraud\tint\n",
      "        tx_fraud_scenario\tint\n",
      "              tx_datetime\ttimestamp\n"
     ]
    }
   ],
   "source": [
    "for r in sorted(df.dtypes, key=lambda x: x[1]):\n",
    "    print(f\"{r[0]:>25}\\t{r[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56f712",
   "metadata": {},
   "source": [
    "Соберем по типам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90276e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "double - 1\n",
      "int    - 7\n",
      "timestamp - 1\n"
     ]
    }
   ],
   "source": [
    "dt.sort(key=lambda x: x[1])\n",
    "\n",
    "print('Data types:')\n",
    "for k, g in groupby(dt, lambda x: x[1]):\n",
    "    print(f'{k:<6} - {len(list(g))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a847e-ad1e-480e-be69-566f32f9ccd2",
   "metadata": {},
   "source": [
    "## Кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f701ba",
   "metadata": {},
   "source": [
    "### Проверка пропущенных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abb02743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк с некорректной суммой операций: 884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|tx_datetime        |customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|32612        |2019-08-22 15:04:07|20987      |461        |0.0      |54247          |0           |0       |0                |\n",
      "|51947        |2019-08-22 07:59:23|33354      |145        |0.0      |28763          |0           |0       |0                |\n",
      "|90921        |2019-08-22 11:40:41|58240      |145        |0.0      |42041          |0           |0       |0                |\n",
      "|111111       |2019-08-22 20:06:49|71171      |228        |0.0      |72409          |0           |0       |0                |\n",
      "|115880       |2019-08-22 13:08:59|74277      |723        |0.0      |47339          |0           |0       |0                |\n",
      "|307118       |2019-08-22 10:47:55|196815     |784        |0.0      |38875          |0           |0       |0                |\n",
      "|357305       |2019-08-22 11:59:19|228980     |157        |0.0      |43159          |0           |0       |0                |\n",
      "|408009       |2019-08-22 08:30:19|261422     |350        |0.0      |30619          |0           |0       |0                |\n",
      "|552481       |2019-08-22 12:42:42|353425     |131        |0.0      |45762          |0           |0       |0                |\n",
      "|593902       |2019-08-22 03:26:36|379977     |998        |0.0      |12396          |0           |0       |0                |\n",
      "|620273       |2019-08-22 10:16:28|396668     |29         |0.0      |36988          |0           |0       |0                |\n",
      "|668329       |2019-08-22 19:12:12|427365     |988        |0.0      |69132          |0           |0       |0                |\n",
      "|672927       |2019-08-22 18:26:37|430305     |944        |0.0      |66397          |0           |0       |0                |\n",
      "|842555       |2019-08-22 17:58:41|538850     |784        |0.0      |64721          |0           |0       |0                |\n",
      "|854278       |2019-08-22 14:16:48|546469     |904        |0.0      |51408          |0           |0       |0                |\n",
      "|861906       |2019-08-22 19:48:45|551333     |81         |0.0      |71325          |0           |0       |0                |\n",
      "|926950       |2019-08-22 16:32:26|592776     |307        |0.0      |59546          |0           |0       |0                |\n",
      "|939180       |2019-08-22 05:24:48|600469     |794        |0.0      |19488          |0           |0       |0                |\n",
      "|943181       |2019-08-22 18:00:56|603037     |409        |0.0      |64856          |0           |0       |0                |\n",
      "|992522       |2019-08-22 11:38:00|634437     |598        |0.0      |41880          |0           |0       |0                |\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк после очистки: 46988417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# поиск некорректной суммы операций (tx_amount <= 0)\n",
    "bad_cond = (F.col(\"tx_amount\") <= 0)\n",
    "bad_count = df.filter(bad_cond).count()\n",
    "\n",
    "print(\"Строк с некорректной суммой операций:\", bad_count)\n",
    "\n",
    "df.filter(bad_cond).show(20, truncate=False)\n",
    "df_clean = df.filter(~bad_cond)\n",
    "\n",
    "print(\"Строк после очистки:\", df.count())\n",
    "df=df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "975f028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк с плохими customer_id/terminal_id: 2041701\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|tx_datetime        |customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|1            |2019-08-22 05:10:37|0          |0          |90.55    |18637          |0           |0       |0                |\n",
      "|2            |2019-08-22 19:05:33|0          |753        |35.38    |68733          |0           |0       |0                |\n",
      "|3            |2019-08-22 07:21:33|0          |0          |80.41    |26493          |0           |0       |0                |\n",
      "|6            |2019-08-22 03:12:21|3          |0          |47.2     |11541          |0           |0       |0                |\n",
      "|12           |2019-08-22 15:47:54|10         |0          |58.89    |56874          |0           |0       |0                |\n",
      "|69           |2019-08-22 12:00:23|44         |0          |70.64    |43223          |0           |0       |0                |\n",
      "|91           |2019-08-22 09:12:18|54         |0          |176.84   |33138          |0           |0       |0                |\n",
      "|107          |2019-08-22 18:45:14|62         |0          |70.68    |67514          |0           |0       |0                |\n",
      "|148          |2019-08-22 10:01:24|95         |0          |16.69    |36084          |0           |0       |0                |\n",
      "|158          |2019-08-22 15:04:06|103        |0          |102.29   |54246          |0           |0       |0                |\n",
      "|221          |2019-08-22 07:32:24|148        |0          |35.13    |27144          |0           |0       |0                |\n",
      "|257          |2019-08-22 12:10:29|174        |0          |34.47    |43829          |0           |0       |0                |\n",
      "|264          |2019-08-22 04:34:54|180        |0          |10.46    |16494          |0           |0       |0                |\n",
      "|265          |2019-08-22 12:59:57|180        |0          |4.01     |46797          |0           |0       |0                |\n",
      "|286          |2019-08-22 07:57:12|196        |0          |43.03    |28632          |0           |0       |0                |\n",
      "|295          |2019-08-22 15:51:01|204        |0          |129.04   |57061          |0           |0       |0                |\n",
      "|297          |2019-08-22 04:09:58|204        |0          |115.22   |14998          |0           |0       |0                |\n",
      "|302          |2019-08-22 11:48:08|206        |0          |29.62    |42488          |0           |0       |0                |\n",
      "|376          |2019-08-22 21:43:12|260        |0          |4.07     |78192          |0           |0       |0                |\n",
      "|392          |2019-08-22 13:06:40|272        |0          |13.24    |47200          |0           |0       |0                |\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк всего: 46987533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:=================================================>     (19 + 2) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк после очистки: 44945832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# customer_id или terminal_id =0 (считаем, что такого не должно быть)\n",
    "bad_cond = (\n",
    "    (F.col(\"customer_id\") == 0) |\n",
    "    (F.col(\"terminal_id\") == 0) |\n",
    "    F.col(\"customer_id\").isNull() |\n",
    "    F.col(\"terminal_id\").isNull()\n",
    ")\n",
    "\n",
    "bad_count = df.filter(bad_cond).count()\n",
    "print(\"Строк с плохими customer_id/terminal_id:\", bad_count)\n",
    "\n",
    "df.filter(bad_cond).show(20, truncate=False)\n",
    "\n",
    "df_clean = df.filter(~bad_cond)\n",
    "\n",
    "# проверка\n",
    "print(\"Строк всего:\", df.count())\n",
    "print(\"Строк после очистки:\", df_clean.count())\n",
    "df=df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3205e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк с некорректными datetime: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|933817       |null       |597125     |611        |62.83    |86400          |0           |0       |0                |\n",
      "|1205236      |null       |769896     |44         |15.1     |86400          |0           |0       |0                |\n",
      "|1543099      |null       |985197     |967        |85.18    |86400          |0           |0       |0                |\n",
      "|1670385      |null       |66533      |956        |21.39    |172800         |1           |0       |0                |\n",
      "|1781446      |null       |137885     |124        |13.7     |172800         |1           |0       |0                |\n",
      "|2615257      |null       |669953     |489        |40.75    |172800         |1           |0       |0                |\n",
      "|2898079      |null       |850307     |532        |14.26    |172800         |1           |0       |0                |\n",
      "|3036595      |null       |938860     |481        |13.52    |172800         |1           |0       |0                |\n",
      "|3837780      |null       |450941     |847        |40.38    |259200         |2           |0       |0                |\n",
      "|4576892      |null       |922295     |241        |59.82    |259200         |2           |0       |0                |\n",
      "|5045996      |null       |221856     |600        |83.95    |345600         |3           |0       |0                |\n",
      "|5263382      |null       |360203     |907        |118.27   |345600         |3           |0       |0                |\n",
      "|5363822      |null       |424141     |227        |16.36    |345600         |3           |0       |0                |\n",
      "|5658834      |null       |612666     |879        |124.16   |345600         |3           |0       |0                |\n",
      "|5742934      |null       |666379     |223        |93.55    |345600         |3           |0       |0                |\n",
      "|7114613      |null       |542697     |507        |31.73    |432000         |4           |0       |0                |\n",
      "|7308521      |null       |666454     |422        |21.44    |432000         |4           |0       |0                |\n",
      "|7400630      |null       |724910     |891        |63.98    |432000         |4           |0       |0                |\n",
      "|7501489      |null       |789414     |29         |81.98    |432000         |4           |0       |0                |\n",
      "|8123142      |null       |186672     |447        |40.24    |518400         |5           |0       |0                |\n",
      "+-------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк всего: 44945832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк после очистки: 44945737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Некорректные datetime\n",
    "\n",
    "bad_cond = (\n",
    "    (F.col(\"tx_datetime\").isNull())\n",
    ")\n",
    "\n",
    "bad_count = df.filter(bad_cond).count()\n",
    "print(\"Строк с некорректными datetime:\", bad_count)\n",
    "\n",
    "df.filter(bad_cond).show(20, truncate=False)\n",
    "\n",
    "df_clean = df.filter(~bad_cond)\n",
    "\n",
    "# проверка\n",
    "print(\"Строк всего:\", df.count())\n",
    "print(\"Строк после очистки:\", df_clean.count())\n",
    "df=df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/13 12:23:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# размещение в backet\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"clean-transactions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "hconf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "\n",
    "\n",
    "hconf.set(\"fs.s3a.endpoint\", \"storage.yandexcloud.net\")\n",
    "hconf.set(\"fs.s3a.access.key\", \"\")\n",
    "hconf.set(\"fs.s3a.secret.key\", \"\")\n",
    "hconf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hconf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "target_path = \"\"\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .mode(\"overwrite\") \n",
    "    .parquet(target_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "400b3186-f554-44a4-9fa7-769706988978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+-------------------+\n",
      "|tranaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|terminal_indexer_id|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+-------------------+\n",
      "|            1|2019-08-22 05:10:37|          0|          0|    90.55|          18637|           0|       0|                0|                0.0|\n",
      "|            2|2019-08-22 19:05:33|          0|        753|    35.38|          68733|           0|       0|                0|               81.0|\n",
      "|            3|2019-08-22 07:21:33|          0|          0|    80.41|          26493|           0|       0|                0|                0.0|\n",
      "|            4|2019-08-22 09:06:17|          1|        981|   102.83|          32777|           0|       0|                0|               45.0|\n",
      "|            5|2019-08-22 18:41:25|          3|        205|     34.2|          67285|           0|       0|                0|              189.0|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "terminal_indexer = StringIndexer(inputCol=\"terminal_id\", outputCol=\"terminal_indexer_id\")\n",
    "terminal_indexer_model = terminal_indexer.fit(df)\n",
    "df_prep = terminal_indexer_model.transform(df)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11ca24",
   "metadata": {},
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.filter(df_prep['Pclass'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6dc5d-2bd1-4b1a-ac6c-e51db02ad939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "pclass_indexer = (\n",
    "    StringIndexer()\n",
    "        .setInputCol('Pclass')\n",
    "        .setOutputCol('PclassIndex')\n",
    ")\n",
    "\n",
    "pclass_indexer_model = pclass_indexer.fit(df_prep)\n",
    "df_prep = pclass_indexer_model.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24996886-643c-45de-9c8b-31d59836afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_encoder = OneHotEncoder()\\\n",
    "    .setInputCol('PclassIndex')\\\n",
    "    .setOutputCol('PclassEncoded')\n",
    "\n",
    "pclass_encoder_model = pclass_encoder.fit(df_prep)\n",
    "df_prep = pclass_encoder_model.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01485340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271258e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.head()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3673d",
   "metadata": {},
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02860d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.filter(df_prep['Embarked'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845bf66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning! Частая ошибка</b>\n",
    "\n",
    "\n",
    "\n",
    "StringIndexer может работать с пропущенными значениями только в формате <b>NaN</b>, но не <b>NULL</b>!\n",
    "\n",
    "Если мы закодируем значения `Embarked` то мы не увидим ошибки. Мы получим ошибку только при обращении к этой строке!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f87fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer()\\\n",
    "    .setInputCol('Embarked')\\\n",
    "    .setOutputCol('EmbarkedIndex')\n",
    "\n",
    "df_err = embark_indexer.fit(df_prep).transform(df_prep)\n",
    "df_err.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a448702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err.show(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaba92",
   "metadata": {},
   "source": [
    "#### Заполним пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859efb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_prep.fillna('X', subset=['Embarked'])\n",
    "df_prep.show(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e64675",
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer()\\\n",
    "    .setInputCol('Embarked')\\\n",
    "    .setOutputCol('EmbarkedIndex')\n",
    "\n",
    "embark_indexer_model = embark_indexer.fit(df_prep)\n",
    "df_prep = embark_indexer_model.transform(df_prep)\n",
    "df_prep.show(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee4497-34b1-4a56-81f2-e87136469127",
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_encoder = OneHotEncoder()\\\n",
    "    .setInputCol('EmbarkedIndex')\\\n",
    "    .setOutputCol('EmbarkedEncoded')\n",
    "\n",
    "embarked_encoder_model = embarked_encoder.fit(df_prep)\n",
    "df_prep = embarked_encoder_model.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47c3a1-c319-482c-990f-3cd508a06cf9",
   "metadata": {},
   "source": [
    "## Нормализация числовых признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e780561",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a75a70-9990-4df0-80f4-c464d2bb2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_imputer = Imputer()\\\n",
    "    .setInputCol('Age')\\\n",
    "    .setOutputCol('AgeImputed')\\\n",
    "    .setStrategy('mean')\n",
    "\n",
    "\n",
    "age_imputer_model = age_imputer.fit(df_prep)\n",
    "df_prep = age_imputer_model.transform(df_prep)\n",
    "df_prep.show(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242fd8f-bb3e-4231-a3ab-e0bf023766fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_assembler = VectorAssembler()\\\n",
    "    .setInputCols([\"AgeImputed\"])\\\n",
    "    .setOutputCol(\"AgeVector\")\n",
    "\n",
    "df_prep = age_assembler.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f81d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a87d0-4fb7-4d78-84e0-2f3d3e9458ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_scaler = MinMaxScaler(inputCol=\"\", outputCol=\"AgeScaled\")\\\n",
    "    .setInputCol('AgeVector')\\\n",
    "    .setOutputCol('AgeScaled')\n",
    "\n",
    "age_scaler_model = age_scaler.fit(df_prep)\n",
    "df_prep = age_scaler_model.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.filter(df_prep['Fare'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_imputer = Imputer(inputCol=\"Fare\", outputCol=\"FareImputed\")\n",
    "fare_imputer_model = fare_imputer.fit(df_prep)\n",
    "df_prep = fare_imputer_model.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d50b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_assembler = VectorAssembler(inputCols=[\"FareImputed\"], outputCol=\"FareVector\")\n",
    "df_prep = fare_assembler.transform(df_prep)\n",
    "df_prep.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f80d4-1586-4cff-a70e-20cf350576a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler\n",
    "\n",
    "fare_scaler = RobustScaler(inputCol=\"FareVector\", outputCol=\"FareScaled\")\n",
    "fare_scaler_model = fare_scaler.fit(df_prep)\n",
    "df_prep = fare_scaler_model.transform(df_prep)\n",
    "df_prep.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db36a7a-2ba4-41dc-b35a-ffd4ee48ff15",
   "metadata": {},
   "source": [
    "## Собираем вектор признаков\n",
    "\n",
    "Для алгоритмов МО из Spark MlLib нужно подавать на вход столбец с вектором признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82def020-a5a5-4d8b-b51d-5589f19bc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features_assembler = VectorAssembler(inputCols=[\n",
    "    \"SexIndex\",\n",
    "    \"PclassEncoded\",\n",
    "    \"AgeScaled\",\n",
    "    \"FareScaled\",\n",
    "    ],\n",
    "    outputCol=\"Features\",\n",
    ")\n",
    "\n",
    "prep_df = df\n",
    "prep_df = sex_indexer_model.transform(prep_df)\n",
    "prep_df = pclass_indexer_model.transform(prep_df)\n",
    "prep_df = pclass_encoder_model.transform(prep_df)\n",
    "prep_df = age_imputer_model.transform(prep_df)\n",
    "prep_df = age_assembler.transform(prep_df)\n",
    "prep_df = age_scaler_model.transform(prep_df)\n",
    "prep_df = fare_imputer_model.transform(prep_df)\n",
    "prep_df = fare_assembler.transform(prep_df)\n",
    "prep_df = fare_scaler_model.transform(prep_df)\n",
    "feat_df = features_assembler.transform(prep_df)\n",
    "\n",
    "feat_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9345ad-3940-47a9-ba7c-8f2366694565",
   "metadata": {},
   "source": [
    "## Конвейер\n",
    "\n",
    "Объединим различные этапы подготовки признаков в единый конвейер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282510-87e9-44db-8c62-730f614f705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "feat_ext_pipe = Pipeline(stages=[\n",
    "    sex_indexer,\n",
    "    pclass_indexer,\n",
    "    pclass_encoder,\n",
    "    age_imputer,\n",
    "    age_assembler,\n",
    "    age_scaler,\n",
    "    fare_imputer_model,\n",
    "    fare_assembler,\n",
    "    fare_scaler_model,\n",
    "    features_assembler,\n",
    "]).fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938218e-c8dd-4440-af04-63ddd99309d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = feat_ext_pipe.transform(df)\n",
    "feat_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301aa7-7a2e-40ff-a614-f4787ac826d0",
   "metadata": {},
   "source": [
    "## Сохранение\n",
    "\n",
    "Сохраним конвейер на диск для последующего использования при подготовке других данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c8ac4-69df-4254-84be-46d721135348",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext_pipe.write().overwrite().save(f\"{app_name}_feat_exty_pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96effa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {app_name}_feat_exty_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826324cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {app_name}_feat_exty_pipe/stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475fe2-2ea6-46e1-a811-b67352b9e593",
   "metadata": {},
   "source": [
    "## Обработка тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a974467-d06e-478f-b9ef-3113bf572edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "test_df = spark.read.csv(\"data/titanic/test.csv\", inferSchema=True, header=True)\n",
    "\n",
    "feat_ext_pipe_loaded = PipelineModel.load(f\"{app_name}_feat_exty_pipe\")\n",
    "\n",
    "prep_test_df = feat_ext_pipe_loaded.transform(test_df)\n",
    "prep_test_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
